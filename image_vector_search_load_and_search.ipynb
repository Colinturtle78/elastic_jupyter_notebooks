{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffvestal/elastic_jupyter_notebooks/blob/main/image_vector_search_load_and_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xoLDtS_6Df1"
      },
      "source": [
        "# !!! WORK IN PROGRESS DO NOT USE YET !!!\n",
        "\n",
        "# Searching for images by description with vector search \n",
        "\n",
        "This code will show you how to set up an ingest pipeline to generate vectors for images on ingest. We will then use kNN vector search to find images matching a description.\n",
        "\n",
        "Overview of steps\n",
        "1. Set up our python environment\n",
        "2. Load model from Hugging Face into Elastic\n",
        "2. Setup index mapping\n",
        "3. Configure ingest pipeline\n",
        "4. Download and unzip [Unsplash Lite Data set](https://github.com/unsplash/datasets#lite-dataset)\n",
        "4. Index an image data set and vectors\n",
        "5. Run a sample kNN search\n",
        "\n",
        "### Requirements\n",
        "This notebook assumes you already have loaded an embedding model into elasticsearch. If you haven't, please start with [this notebook example](https://github.com/jeffvestal/elastic_jupyter_notebooks/blob/main/load_embedding_model_from_hf_to_elastic.ipynb)\n",
        "\n",
        "\n",
        "### Elastic version support\n",
        "Requires Elastic version 8.0+ with a platinum or enterprise license (or trial license)\n",
        "\n",
        "You can set up a [free trial elasticsearch Deployment in Elastic Cloud](https://cloud.elastic.co/registration)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgxCKQS7mCZw"
      },
      "source": [
        "---\n",
        "---\n",
        "# Setup\n",
        "---\n",
        "---\n",
        "This section will set up the python environment with the required libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1f1P-l9ri8"
      },
      "source": [
        "## Install and import required python libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJAb_8zlPFhQ"
      },
      "source": [
        "Elastic uses the [eland python library](https://github.com/elastic/eland) to download modesl from Hugging Face hub and load them into elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUedSzQW9FIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "253f9015-4046-4f28-c9a9-f72b16819f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: eland in /usr/local/lib/python3.8/dist-packages (8.3.0)\n",
            "Requirement already satisfied: pandas<2,>=1.2 in /usr/local/lib/python3.8/dist-packages (from eland) (1.3.5)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.8/dist-packages (from eland) (3.2.2)\n",
            "Requirement already satisfied: elasticsearch<9,>=8.3 in /usr/local/lib/python3.8/dist-packages (from eland) (8.6.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.8/dist-packages (from eland) (1.21.6)\n",
            "Requirement already satisfied: elastic-transport<9,>=8 in /usr/local/lib/python3.8/dist-packages (from elasticsearch<9,>=8.3->eland) (8.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->eland) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->eland) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->eland) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->eland) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<2,>=1.2->eland) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<2,>=1.26.2 in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch<9,>=8.3->eland) (1.26.14)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch<9,>=8.3->eland) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib<4->eland) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install eland"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK3Wx1I199yB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad22d12-78ee-4364-cf24-6a176ef65edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.8/dist-packages (8.6.1)\n",
            "Requirement already satisfied: elastic-transport<9,>=8 in /usr/local/lib/python3.8/dist-packages (from elasticsearch) (8.4.0)\n",
            "Requirement already satisfied: urllib3<2,>=1.26.2 in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.14)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "pip install elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEfiiFXakzdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97822c2-cbb2-4e32-f5d9-2785055346e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I20mDmJboKZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1dfb85-e517-495e-98ba-61529d675c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.26.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.11.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Collecting torch>=1.6.0\n",
            "  Downloading torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.38.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (4.0.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
          ]
        }
      ],
      "source": [
        "pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqcpWrbkBEB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5759ba6-2ccc-4204-96a7-96f674501973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.11\n",
            "  Using cached torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11) (4.4.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1\n",
            "    Uninstalling torch-1.13.1:\n",
            "      Successfully uninstalled torch-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n"
          ]
        }
      ],
      "source": [
        "pip install torch==1.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyUZXUi4RWWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4e1188-2aab-4de1-b0ef-cd560c9bdb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b1a7c4bb8212>:5: DeprecationWarning: Importing from the 'elasticsearch.client' module is deprecated. Instead use 'elasticsearch' module for importing the client.\n",
            "  from elasticsearch.client import MlClient\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from eland.ml.pytorch import PyTorchModel\n",
        "from eland.ml.pytorch.transformers import TransformerModel\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "from elasticsearch.client import MlClient\n",
        "from pprint import pprint\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "import pandas as pd\n",
        "import base64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7nMIbHke37Q"
      },
      "source": [
        "## Configure elasticsearch authentication. \n",
        "The recommended authentication approach is using the [Elastic Cloud ID](https://www.elastic.co/guide/en/cloud/current/ec-cloud-id.html) and a [cluster level API key](https://www.elastic.co/guide/en/kibana/current/api-keys.html)\n",
        "\n",
        "You can use any method you wish to set the required credentials. We are using getpass in this example to prompt for credentials to avoide storing them in github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsd2m7HoTCLm"
      },
      "outputs": [],
      "source": [
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSGgYHome69o"
      },
      "outputs": [],
      "source": [
        "es_cloud_id = getpass.getpass('Enter Elastic Cloud ID:  ')\n",
        "es_api_id = getpass.getpass('Enter cluster API key ID:  ') \n",
        "es_api_key = getpass.getpass('Enter cluster API key:  ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL4VDnVp96lf"
      },
      "source": [
        "## Connect to Elastic Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8mVJkKmetXo"
      },
      "outputs": [],
      "source": [
        "es = Elasticsearch(cloud_id=es_cloud_id, \n",
        "                   api_key=(es_api_id, es_api_key)\n",
        "                   )\n",
        "es.info() # should return cluster info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UYSzFp3vHdB"
      },
      "source": [
        "---\n",
        "---\n",
        "# Load Model from Hugging Face\n",
        "---\n",
        "---\n",
        "We will be using the [clip-ViT-B-32-multilingual-v1](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1) sentence transformers model. This model's capabilities are described as:\n",
        "\n",
        "> You can map text (in 50+ languages) and images to a common dense vector space such that images and the matching texts are close. This model can be used for image search (users search through a large collection of images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the Model\n",
        "Here we specify the model id from Hugging Face. The easiest way to get this id is clicking the copy the model name icon next to the name on the model page. \n",
        "\n",
        "When calling `TransformerModel` you specify the HF model id and the task type. You can try specifying `auto` and eland will attempt to determine the correct type from info in the model config. This is not always possible so a list of specific `task_type` values can be viewed in the following code: \n",
        "[Supported values](https://github.com/elastic/eland/blob/15a300728876022b206161d71055c67b500a0192/eland/ml/pytorch/transformers.py#*L41*)"
      ],
      "metadata": {
        "id": "uBMWHj-ZmtvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model_id='sentence-transformers/clip-ViT-B-32-multilingual-v1'\n",
        "tm = TransformerModel(hf_model_id, \"text_embedding\")"
      ],
      "metadata": {
        "id": "zPV3oFsKiYFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set and confirm the model ID\n",
        "To make the name compatible with elasticsearch, the '/' is replaced with '__'\n",
        "\n"
      ],
      "metadata": {
        "id": "sX-9dHuDmwgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es_model_id = tm.elasticsearch_model_id()\n",
        "es_model_id"
      ],
      "metadata": {
        "id": "XkIQBBCbdqvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the model in a TorchScrpt representation which Elasticsearch uses"
      ],
      "metadata": {
        "id": "p0L2cfYwbIld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_path = \"models\"\n",
        "Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
        "model_path, config, vocab_path = tm.save(tmp_path)"
      ],
      "metadata": {
        "id": "GsSpvvP-nbCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model into Elasticsearch\n",
        "Model should not already exist in elasticsearch"
      ],
      "metadata": {
        "id": "k1a_yNo6ba2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ptm = PyTorchModel(es, es_model_id)\n",
        "ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config) "
      ],
      "metadata": {
        "id": "Z4QD71Apnj4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting the Model"
      ],
      "metadata": {
        "id": "xUVf0YpexcgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View information about the model\n",
        "This is not required but can be handy to get a model overivew"
      ],
      "metadata": {
        "id": "Ki9n3LgLxcgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = MlClient.get_trained_models(es, model_id=es_model_id)\n",
        "m.body"
      ],
      "metadata": {
        "id": "KF3hkIjmxcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy the model\n",
        "This will load the model on the ML nodes and start the process(es) making it available for the NLP task"
      ],
      "metadata": {
        "id": "Xiq-c71CxcgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = MlClient.start_trained_model_deployment(es, model_id=es_model_id)\n",
        "s.body"
      ],
      "metadata": {
        "id": "v63zdzeoxcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the model started without issue"
      ],
      "metadata": {
        "id": "MOYkF9trxcgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats = MlClient.get_trained_models_stats(es, model_id=es_model_id)\n",
        "stats.body['trained_model_stats'][0]['deployment_stats']['nodes'][0]['routing_state']"
      ],
      "metadata": {
        "id": "SOn3byT1xcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEwsReS8zyOc"
      },
      "source": [
        "---\n",
        "---\n",
        "# Elasticsearch index setup\n",
        "---\n",
        "---\n",
        "Here we will configure an index template with settings and mappings to store our vectors and text data\n",
        "\n",
        "The **important** part here will be setting our vector field to be a `dense_vector` type. This will tell elasticsearch to build the HNSW graph for the vectors so we can then use kNN search later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQvNTOJQ2Jk1"
      },
      "source": [
        "## Define the index template\n",
        "We will have the following fields\n",
        "\n",
        "- `vectors` of type `dense_vector`\n",
        "-- it is important to set `dims` to the number of dimensions the model you will use outputs\n",
        "- `title` of type `text`\n",
        "- `summary` of type `text`\n",
        "\n",
        "We will have \n",
        "- 1 primary shard\n",
        "- 0 replica -> *note* in production you will want at least 1 replica\n",
        "\n",
        "This will match new indices with the name matching the pattern of `jupyter-vector-demo*`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8079Ic44SEO"
      },
      "outputs": [],
      "source": [
        "index_id = 'jupyter-vector-image-search-demo'\n",
        "index_patterns = index_id + \"*\"\n",
        "settings= {\n",
        "        \"number_of_shards\": 1,\n",
        "        \"number_of_replicas\": 0\n",
        "    }\n",
        "mappings= {\n",
        "        \"properties\": {\n",
        "            \"vectors\": {\n",
        "                \"type\": \"dense_vector\",\n",
        "                \"dims\": 512,\n",
        "                \"index\" : True,\n",
        "                \"similarity\" : \"cosine\"\n",
        "            },\n",
        "            \"title\": {\n",
        "                \"type\": \"text\"\n",
        "            },\n",
        "            \"summary\": {\n",
        "                \"type\": \"text\"\n",
        "            }\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vryddQGB3U6q"
      },
      "source": [
        "## Apply the template\n",
        "Here we apply the templat and give it a name of `jupyter-vector-demo`. This is just the name of the template if we need to modify it later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEmOQ4IT3XK8"
      },
      "outputs": [],
      "source": [
        "template_id = index_id + '-template'\n",
        "es.indices.put_template(name=template_id, \n",
        "                        index_patterns=index_patterns,\n",
        "                        settings=settings,\n",
        "                        mappings=mappings\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MZ6EBVUTjhN"
      },
      "source": [
        "---\n",
        "---\n",
        "# The Ingest Pipeline\n",
        "---\n",
        "---\n",
        "\n",
        "An ingest pipeline has one or more processors and processes documents before they are written into an elasticsearch index. \n",
        "\n",
        "Each processor is designed to perform a various task such as parsing fields or enriching data. \n",
        "\n",
        "The main processor for this pipeline is the `inference` processor. The inference processor sends a specified field to a supervised model and writes the output from the model to a new field along with the original fields in the document. \n",
        "\n",
        "To make it simpler to access the vector, we will copy the vectors to a field named `vectors` and them remove the `ml` field tree which is the default output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iOh80S0UhsU"
      },
      "source": [
        "## Configure the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwwyOBWEVd-P"
      },
      "outputs": [],
      "source": [
        "pipeline_definition = {\n",
        "    \"description\": \"A pipeline for generating and storing vectors on ingest\",\n",
        "    \"processors\": [\n",
        "      {\n",
        "       \"inference\": {\n",
        "          \"model_id\": es_model_id,\n",
        "          \"field_map\": {\n",
        "           \"summary\": \"text_field\"\n",
        "          }\n",
        "       }\n",
        "     },\n",
        "     {\n",
        "      \"set\": {\n",
        "        \"field\": \"vectors\",\n",
        "        \"copy_from\": \"ml.inference.predicted_value\"\n",
        "        }\n",
        "     },\n",
        "    {\n",
        "      \"remove\": {\n",
        "        \"field\": \"ml\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHzxqcvIVjyC"
      },
      "source": [
        "## Create the pipeline if it doesn't exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXZUQI3IVp21"
      },
      "outputs": [],
      "source": [
        "pipeline_id = index_id + '-pipeline'\n",
        "if es.ingest.put_pipeline(id=pipeline_id, body=pipeline_definition):\n",
        "    print(\"Pipeline created successfully\")\n",
        "else:\n",
        "    print(\"Failed to create pipeline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRyS-1HjcqV7"
      },
      "source": [
        "## Verify the pipeline\n",
        "Not required but nice to verify everything looks correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH59icc9czcD"
      },
      "outputs": [],
      "source": [
        "pipeline = es.ingest.get_pipeline(id=pipeline_id)\n",
        "pipeline.body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruM78vW_hTOy"
      },
      "source": [
        "---\n",
        "---\n",
        "# Ingest Docs and Generate Vectors\n",
        "---\n",
        "---\n",
        "We will be using [Usplash Lite dataset](https://github.com/unsplash/datasets#lite-dataset). \n",
        "\n",
        "*note* the data set size: [~650MB compressed, ~1.4GB raw]. This will fit in the storage space supplied by the colab space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY3kxbN_hYGC"
      },
      "source": [
        "## Download the images dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://unsplash.com/data/lite/latest\"\n",
        "r = requests.get(url)"
      ],
      "metadata": {
        "id": "ry9UwDMW4S4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open the zip file"
      ],
      "metadata": {
        "id": "JN8IeH-t4byy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = zipfile.ZipFile(io.BytesIO(r.content))"
      ],
      "metadata": {
        "id": "G_ub5gvl4cF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract the contents of the zip file to a folder"
      ],
      "metadata": {
        "id": "73wyvl0G4e-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z.extractall(\"unsplash_images\")"
      ],
      "metadata": {
        "id": "wTYgbE6g4fcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check a sample image\n",
        "the path here is where the files will extract to in google colab "
      ],
      "metadata": {
        "id": "_6HrcHTeB-WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/unsplash_images/photos.tsv000'\n",
        "df = pd.read_csv(filename, sep='\\t', header=0)\n"
      ],
      "metadata": {
        "id": "b6rdsO1vDdJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check a sampling data (selecting only the columns we need)"
      ],
      "metadata": {
        "id": "H-LV3l2wHEHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['photo_id', 'photo_image_url', 'ai_description']]"
      ],
      "metadata": {
        "id": "OlfdAnbTDt2p",
        "outputId": "cb42c1cd-b45c-4331-f903-c7705dd86d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          photo_id                                    photo_image_url  \\\n",
              "0      XMyPniM9LF0  https://images.unsplash.com/uploads/1411949294...   \n",
              "1      rDLBArZUl1c  https://images.unsplash.com/photo-141633941111...   \n",
              "2      cNDGZ2sQ3Bo  https://images.unsplash.com/photo-142014251503...   \n",
              "3      iuZ_D1eoq9k  https://images.unsplash.com/photo-141487280988...   \n",
              "4      BeD3vjQ8SI0  https://images.unsplash.com/photo-141700759404...   \n",
              "...            ...                                                ...   \n",
              "24995  c7OrOMxrurA  https://images.unsplash.com/photo-159300793778...   \n",
              "24996  15IuQ5a0Qwg  https://images.unsplash.com/photo-159296761254...   \n",
              "24997  w8nrcXz8pwk  https://images.unsplash.com/photo-159299937329...   \n",
              "24998  n1jHrRhehUI  https://images.unsplash.com/photo-159192792878...   \n",
              "24999  Ic74ACoaAX0  https://images.unsplash.com/photo-159240763188...   \n",
              "\n",
              "                                          ai_description  \n",
              "0                  woman walking in the middle of forest  \n",
              "1              succulent plants in clear glass terrarium  \n",
              "2               rocky mountain under gray sky at daytime  \n",
              "3      red common poppy flower selective focus phography  \n",
              "4                                trees during night time  \n",
              "...                                                  ...  \n",
              "24995                   black metal fence during daytime  \n",
              "24996          white and brown seashell on white surface  \n",
              "24997         leopard on brown tree trunk during daytime  \n",
              "24998  woman in beige coat and white hat standing on ...  \n",
              "24999  green plants on brown rocky mountain under blu...  \n",
              "\n",
              "[25000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc6dba5d-554b-4341-b43a-c99a477e4a20\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>photo_id</th>\n",
              "      <th>photo_image_url</th>\n",
              "      <th>ai_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XMyPniM9LF0</td>\n",
              "      <td>https://images.unsplash.com/uploads/1411949294...</td>\n",
              "      <td>woman walking in the middle of forest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rDLBArZUl1c</td>\n",
              "      <td>https://images.unsplash.com/photo-141633941111...</td>\n",
              "      <td>succulent plants in clear glass terrarium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cNDGZ2sQ3Bo</td>\n",
              "      <td>https://images.unsplash.com/photo-142014251503...</td>\n",
              "      <td>rocky mountain under gray sky at daytime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>iuZ_D1eoq9k</td>\n",
              "      <td>https://images.unsplash.com/photo-141487280988...</td>\n",
              "      <td>red common poppy flower selective focus phography</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BeD3vjQ8SI0</td>\n",
              "      <td>https://images.unsplash.com/photo-141700759404...</td>\n",
              "      <td>trees during night time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>c7OrOMxrurA</td>\n",
              "      <td>https://images.unsplash.com/photo-159300793778...</td>\n",
              "      <td>black metal fence during daytime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>15IuQ5a0Qwg</td>\n",
              "      <td>https://images.unsplash.com/photo-159296761254...</td>\n",
              "      <td>white and brown seashell on white surface</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>w8nrcXz8pwk</td>\n",
              "      <td>https://images.unsplash.com/photo-159299937329...</td>\n",
              "      <td>leopard on brown tree trunk during daytime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>n1jHrRhehUI</td>\n",
              "      <td>https://images.unsplash.com/photo-159192792878...</td>\n",
              "      <td>woman in beige coat and white hat standing on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>Ic74ACoaAX0</td>\n",
              "      <td>https://images.unsplash.com/photo-159240763188...</td>\n",
              "      <td>green plants on brown rocky mountain under blu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc6dba5d-554b-4341-b43a-c99a477e4a20')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc6dba5d-554b-4341-b43a-c99a477e4a20 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc6dba5d-554b-4341-b43a-c99a477e4a20');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build list of docs to index\n",
        "\n",
        "We need to accomplish two tasks at this step\n",
        "1. download an image \n",
        "2. convert it to base64 (this will be passed to the clip model to generate a vector)\n",
        "3. create the payload for the bulk insert"
      ],
      "metadata": {
        "id": "-z81xTc8OY2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "sample_count = 100\n",
        "\n",
        "for i, row in df.head(sample_count).iterrows():\n",
        "  photo_image_url = row['photo_image_url']\n",
        "\n",
        "  response = requests.get(photo_image_url)\n",
        "  img = response.content\n",
        "  b64 = base64.b64encode(img)\n",
        "  b64_string = b64.decode(\"utf-8\")\n",
        "\n",
        "\n",
        "  docs.append(\n",
        "    {   \"_index\": index_id,\n",
        "        \"_source\": {\n",
        "          'photo_image_url' : photo_image_url,\n",
        "          'photo_id' : row['photo_id'],\n",
        "          'ai_description' : row['ai_description']\n",
        "        }\n",
        "    }\n",
        "  )\n",
        "\n"
      ],
      "metadata": {
        "id": "P5Lcp8yhIlfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmBfO3N6nf37"
      },
      "source": [
        "## Index the docs \n",
        "This will send a bulk index request to elastic, sending all the docs through the ingest pipeline, generating vectors, and storing them in elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jyd_xOyo4xK"
      },
      "outputs": [],
      "source": [
        "helpers.bulk(es, docs, pipeline=pipieline_id )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xZ8SyBTpOYh"
      },
      "source": [
        "## Verify one of the docs \n",
        "Let's take a look at one doc and see how it was indexed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EasfA-uyqnUu"
      },
      "outputs": [],
      "source": [
        "result = es.search(index=index_id, body={}, size=1)\n",
        "result.body['hits']['hits'][0]['_source']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M-4Fc-zn7D_"
      },
      "source": [
        "---\n",
        "---\n",
        "# Searching for an image by text description\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hu2n4bmGYkG"
      },
      "source": [
        "## Generate Vector for Query\n",
        "\n",
        "Before we can run an approximate k-nearest neighbor (kNN) query, we need to convert our query string to a vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXAQXIS0pvfR"
      },
      "source": [
        "Set a sample query doc\n",
        "\n",
        "Depending on your specific model, you may need to change the field name from \"text_field\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBNV7q5Dwlz6"
      },
      "outputs": [],
      "source": [
        "docs =  [\n",
        "    {\n",
        "      \"text_field\": \"State of the art nlp models\"\n",
        "    }\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5_BYcpvrLsb"
      },
      "source": [
        "We call the `_infer` endpoint supplying the model_id and the doc[s] we want to vectorize. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsWg7XPSGbiu"
      },
      "outputs": [],
      "source": [
        "vec = MlClient.infer_trained_model(es, model_id=es_model_id, docs=docs, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdC3PkTyrZEV"
      },
      "source": [
        "The vector for the first doc can be accessed in the response dict as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rle3J5mJXbdf"
      },
      "outputs": [],
      "source": [
        "doc_0_vector = vec['inference_results'][0]['predicted_value']\n",
        "doc_0_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyeoQ8TK8Ddr"
      },
      "source": [
        "## Run the Search\n",
        "\n",
        "We will call the `_search` api and specify the `knn` section. \n",
        "\n",
        "This is a simple example of a search query. Elastic supports combining kNN search with \"traditional\" BM25 search. You can also filter documents to reduce the number of docs that needs to be searched. See the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search-api.html) for more information.\n",
        "\n",
        "This will be a very simple example to get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31c_Z4eL8mou"
      },
      "source": [
        "### Create the search paramaters\n",
        "Here we are just specifying the `knn` section, but you can also set all the other search params to pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xI-NOjv9bbP"
      },
      "outputs": [],
      "source": [
        "knn = {\n",
        "    \"field\": \"vectors\",\n",
        "    \"query_vector\": doc_0_vector,\n",
        "    \"k\": 2,\n",
        "    \"num_candidates\": 10\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXFxIpJxQTol"
      },
      "source": [
        "Send in the search request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwQoXata8pZY"
      },
      "outputs": [],
      "source": [
        "results = es.search(index='jupyter-vector-demo', knn=knn, size=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmST0UL1Eqv2"
      },
      "source": [
        "### View the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAoKozJWEvto"
      },
      "outputs": [],
      "source": [
        "results.body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mewh4SoLFBlM"
      },
      "source": [
        "### Search Query without returning vectors\n",
        "\n",
        "Often when running kNN search, you don't actually need to return the vectors themselves, you just want to return the fields to display to the end user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_xGJ6wQed4"
      },
      "source": [
        "We are adding a new paramater `fields` which allows us to specify specific fields to return rather than all of them\n",
        "\n",
        "By setting `source` to False (_source:false) we save having to get the entire source payoad back in the response\n",
        "\n",
        "We are moving the `size` value here simply to gather the paramaters together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgV6DIoCFBlM"
      },
      "outputs": [],
      "source": [
        "knn = {\n",
        "    \"field\": \"vectors\",\n",
        "    \"query_vector\": doc_0_vector,\n",
        "    \"k\": 2,\n",
        "    \"num_candidates\": 10\n",
        "  }\n",
        "fields = [\"summary\", \"title\"]\n",
        "size = 1\n",
        "source = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl9ys10oFBlM"
      },
      "outputs": [],
      "source": [
        "results = es.search(index='jupyter-vector-demo', \n",
        "                    knn=knn, \n",
        "                    source=source, \n",
        "                    fields=fields, \n",
        "                    size=size\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqHp2PMuFBlN"
      },
      "source": [
        "### View the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqgB31ZdFBlN"
      },
      "outputs": [],
      "source": [
        "results.body"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Searching for images by description with vector search",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHaofoskvciFWGSsRnNzAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}